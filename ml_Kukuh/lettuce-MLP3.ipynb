{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_series(data, min, max):\n",
    "    data-=min\n",
    "    data/=max\n",
    "    return data\n",
    "\n",
    "def windowed_dataset(series, batch_size, n_past=24, n_future=4, shift=1):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(size=n_past+n_future, shift=shift, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(n_past+n_future))\n",
    "    ds = ds.shuffle(1000)\n",
    "    ds = ds.map(lambda w: (w[:-n_future], w[-n_future:, :1]))\n",
    "    ds = ds.batch(batch_size).prefetch(shift)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Temperature (Â°C)  Humidity (%)  TDS Value (ppm)  pH Level  Growth Days\n",
      "0              33.4            53              582       6.4            1\n",
      "1              33.5            53              451       6.1            2\n",
      "2              33.4            59              678       6.4            3\n",
      "3              33.4            68              420       6.4            4\n",
      "4              33.4            74              637       6.5            5\n",
      "n features: 5\n"
     ]
    }
   ],
   "source": [
    "# Retrieve dataset\n",
    "df = pd.read_csv(os.path.join('./dataset/2/lettuce_dataset.csv'), sep=',', encoding='ISO-8859-1')\n",
    "\n",
    "# Delete column date, plant_id\n",
    "df = df.drop(columns=['Plant_ID', 'Date'])\n",
    "\n",
    "# Number of features in dataset\n",
    "N_FEATURES = df.shape[1]\n",
    "\n",
    "print(df.head())\n",
    "print(F'n features: {N_FEATURES}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2535\n",
      "Test: 634\n"
     ]
    }
   ],
   "source": [
    "# Normalize data\n",
    "data = df.values\n",
    "split_time = int(len(data)*0.8)\n",
    "data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
    "\n",
    "x_train = data[:split_time]\n",
    "print(f'Train: {len(x_train)}')\n",
    "x_valid = data[split_time:]\n",
    "print(f'Test: {len(x_valid)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\APPLICATION\\APP\\Anaconda\\main\\envs\\Bangkit\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\APPLICATION\\APP\\Anaconda\\main\\envs\\Bangkit\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:From d:\\APPLICATION\\APP\\Anaconda\\main\\envs\\Bangkit\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\APPLICATION\\APP\\Anaconda\\main\\envs\\Bangkit\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "      1/Unknown - 1s 1s/step - loss: 0.3662 - mae: 0.3662WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0008s vs `on_train_batch_end` time: 0.0014s). Check your callbacks.\n",
      "78/78 [==============================] - 2s 7ms/step - loss: 0.1281 - mae: 0.1281 - val_loss: 0.1361 - val_mae: 0.1361\n",
      "Epoch 2/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0652 - mae: 0.0652 - val_loss: 0.1254 - val_mae: 0.1254\n",
      "Epoch 3/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0642 - mae: 0.0642 - val_loss: 0.1342 - val_mae: 0.1342\n",
      "Epoch 4/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0638 - mae: 0.0638 - val_loss: 0.1376 - val_mae: 0.1376\n",
      "Epoch 5/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0639 - mae: 0.0639 - val_loss: 0.1550 - val_mae: 0.1550\n",
      "Epoch 6/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0634 - mae: 0.0634 - val_loss: 0.1604 - val_mae: 0.1604\n",
      "Epoch 7/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0633 - mae: 0.0633 - val_loss: 0.1764 - val_mae: 0.1764\n",
      "Epoch 8/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0631 - mae: 0.0631 - val_loss: 0.1773 - val_mae: 0.1773\n",
      "Epoch 9/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0629 - mae: 0.0629 - val_loss: 0.1693 - val_mae: 0.1693\n",
      "Epoch 10/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0633 - mae: 0.0633 - val_loss: 0.1886 - val_mae: 0.1886\n",
      "Epoch 11/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0631 - mae: 0.0631 - val_loss: 0.2003 - val_mae: 0.2003\n",
      "Epoch 12/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0629 - mae: 0.0629 - val_loss: 0.2016 - val_mae: 0.2016\n",
      "Epoch 13/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0628 - mae: 0.0628 - val_loss: 0.2065 - val_mae: 0.2065\n",
      "Epoch 14/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0628 - mae: 0.0628 - val_loss: 0.2147 - val_mae: 0.2147\n",
      "Epoch 15/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0626 - mae: 0.0626 - val_loss: 0.2032 - val_mae: 0.2032\n",
      "Epoch 16/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0629 - mae: 0.0629 - val_loss: 0.2136 - val_mae: 0.2136\n",
      "Epoch 17/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0627 - mae: 0.0627 - val_loss: 0.2181 - val_mae: 0.2181\n",
      "Epoch 18/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0626 - mae: 0.0626 - val_loss: 0.2280 - val_mae: 0.2280\n",
      "Epoch 19/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0624 - mae: 0.0624 - val_loss: 0.2260 - val_mae: 0.2260\n",
      "Epoch 20/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0625 - mae: 0.0625 - val_loss: 0.2272 - val_mae: 0.2272\n",
      "Epoch 21/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0624 - mae: 0.0624 - val_loss: 0.2319 - val_mae: 0.2319\n",
      "Epoch 22/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0623 - mae: 0.0623 - val_loss: 0.2391 - val_mae: 0.2391\n",
      "Epoch 23/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0623 - mae: 0.0623 - val_loss: 0.2386 - val_mae: 0.2386\n",
      "Epoch 24/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0623 - mae: 0.0623 - val_loss: 0.2416 - val_mae: 0.2416\n",
      "Epoch 25/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0623 - mae: 0.0623 - val_loss: 0.2280 - val_mae: 0.2280\n",
      "Epoch 26/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0624 - mae: 0.0624 - val_loss: 0.2409 - val_mae: 0.2409\n",
      "Epoch 27/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0623 - mae: 0.0623 - val_loss: 0.2297 - val_mae: 0.2297\n",
      "Epoch 28/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0624 - mae: 0.0624 - val_loss: 0.2425 - val_mae: 0.2425\n",
      "Epoch 29/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0623 - mae: 0.0623 - val_loss: 0.2377 - val_mae: 0.2377\n",
      "Epoch 30/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0622 - mae: 0.0622 - val_loss: 0.2351 - val_mae: 0.2351\n",
      "Epoch 31/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0623 - mae: 0.0623 - val_loss: 0.2391 - val_mae: 0.2391\n",
      "Epoch 32/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0621 - mae: 0.0621 - val_loss: 0.2466 - val_mae: 0.2466\n",
      "Epoch 33/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0622 - mae: 0.0622 - val_loss: 0.2428 - val_mae: 0.2428\n",
      "Epoch 34/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0622 - mae: 0.0622 - val_loss: 0.2470 - val_mae: 0.2470\n",
      "Epoch 35/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0622 - mae: 0.0622 - val_loss: 0.2377 - val_mae: 0.2377\n",
      "Epoch 36/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0623 - mae: 0.0623 - val_loss: 0.2481 - val_mae: 0.2481\n",
      "Epoch 37/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0621 - mae: 0.0621 - val_loss: 0.2424 - val_mae: 0.2424\n",
      "Epoch 38/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0622 - mae: 0.0622 - val_loss: 0.2355 - val_mae: 0.2355\n",
      "Epoch 39/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0623 - mae: 0.0623 - val_loss: 0.2466 - val_mae: 0.2466\n",
      "Epoch 40/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0620 - mae: 0.0620 - val_loss: 0.2443 - val_mae: 0.2443\n",
      "Epoch 41/60\n",
      "78/78 [==============================] - 0s 5ms/step - loss: 0.0622 - mae: 0.0622 - val_loss: 0.2535 - val_mae: 0.2535\n",
      "Epoch 42/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0621 - mae: 0.0621 - val_loss: 0.2440 - val_mae: 0.2440\n",
      "Epoch 43/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0623 - mae: 0.0623 - val_loss: 0.2393 - val_mae: 0.2393\n",
      "Epoch 44/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0623 - mae: 0.0623 - val_loss: 0.2495 - val_mae: 0.2495\n",
      "Epoch 45/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0621 - mae: 0.0621 - val_loss: 0.2494 - val_mae: 0.2494\n",
      "Epoch 46/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0621 - mae: 0.0621 - val_loss: 0.2536 - val_mae: 0.2536\n",
      "Epoch 47/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0621 - mae: 0.0621 - val_loss: 0.2564 - val_mae: 0.2564\n",
      "Epoch 48/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0619 - mae: 0.0619 - val_loss: 0.2435 - val_mae: 0.2435\n",
      "Epoch 49/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0622 - mae: 0.0622 - val_loss: 0.2555 - val_mae: 0.2555\n",
      "Epoch 50/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0620 - mae: 0.0620 - val_loss: 0.2564 - val_mae: 0.2564\n",
      "Epoch 51/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0620 - mae: 0.0620 - val_loss: 0.2444 - val_mae: 0.2444\n",
      "Epoch 52/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0623 - mae: 0.0623 - val_loss: 0.2507 - val_mae: 0.2507\n",
      "Epoch 53/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0620 - mae: 0.0620 - val_loss: 0.2485 - val_mae: 0.2485\n",
      "Epoch 54/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0621 - mae: 0.0621 - val_loss: 0.2521 - val_mae: 0.2521\n",
      "Epoch 55/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0621 - mae: 0.0621 - val_loss: 0.2607 - val_mae: 0.2607\n",
      "Epoch 56/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0619 - mae: 0.0619 - val_loss: 0.2503 - val_mae: 0.2503\n",
      "Epoch 57/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0620 - mae: 0.0620 - val_loss: 0.2447 - val_mae: 0.2447\n",
      "Epoch 58/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0623 - mae: 0.0623 - val_loss: 0.2534 - val_mae: 0.2534\n",
      "Epoch 59/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0619 - mae: 0.0619 - val_loss: 0.2520 - val_mae: 0.2520\n",
      "Epoch 60/60\n",
      "78/78 [==============================] - 0s 4ms/step - loss: 0.0620 - mae: 0.0620 - val_loss: 0.2424 - val_mae: 0.2424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x13bbc03f6d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS\n",
    "BATCH_SIZE = 32\n",
    "N_PAST = 24  # Number of past time steps based on which future observations should be predicted\n",
    "N_FUTURE = 24  # Number of future time steps which are to be predicted.\n",
    "SHIFT = 1  # By how many positions the window slides to create a new window of observations.\n",
    "\n",
    "# Code to create windowed train and validation datasets.\n",
    "# Complete the code in windowed_dataset.\n",
    "# YOUR CODE HERE\n",
    "train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE, n_past=N_PAST, n_future=N_FUTURE, shift=SHIFT)\n",
    "# YOUR CODE HERE\n",
    "valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE, n_past=N_PAST, n_future=N_FUTURE, shift=SHIFT)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # tf.keras.layers.LSTM(64, 'relu', return_sequences=True, input_shape=[N_PAST, N_FEATURES]),\n",
    "    tf.keras.layers.Dense(64, input_shape=(N_PAST, N_FEATURES)),\n",
    "    tf.keras.layers.Dense(32),\n",
    "    tf.keras.layers.Dense(N_FUTURE, 'linear'),\n",
    "])\n",
    "\n",
    "\n",
    "class StopWhenReachDesireMAE(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, monitor='mae', monitor2='val_mae', target=0.14):\n",
    "        super(StopWhenReachDesireMAE, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.monitor2 = monitor2\n",
    "        self.target = target\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        current2 = logs.get(self.monitor2)\n",
    "        if current is not None and current2 is not None:\n",
    "            if current < self.target and current2 < self.target:\n",
    "                print(\n",
    "                    f'\\nEpoch {epoch + 1}: {self.monitor} and {self.monitor2} have reached {self.target}. Stopping training.')\n",
    "                self.model.stop_training = True\n",
    "\n",
    "stop_callback = StopWhenReachDesireMAE('mae', 'val_mae', 0.05)\n",
    "\n",
    "# Code to train and compile the model\n",
    "# YOUR CODE HERE\n",
    "model.compile(\n",
    "    loss='mae',\n",
    "    optimizer='adam',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_set,\n",
    "    epochs=60,\n",
    "    validation_data=valid_set,\n",
    "    callbacks=[stop_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model\n",
    "# loss, mae = model.evaluate(X_test, y_test)\n",
    "# print(f'Mean Absolute Error on test data: {mae}')\n",
    "# \n",
    "# # Make predictions\n",
    "# y_pred = model.predict(X_test)\n",
    "# \n",
    "# # Optional: Convert predictions and actual values to a more readable format\n",
    "# results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred.flatten()})\n",
    "# print(results.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bangkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
